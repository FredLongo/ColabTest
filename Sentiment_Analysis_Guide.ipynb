{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsp+RHqQGwPIj2xSbi7PpH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FredLongo/ColabTest/blob/main/Sentiment_Analysis_Guide.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vK-I_NZT4dU0",
        "outputId": "03e7a656-379f-4811-cbb5-fe2e05107980"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to retrieve the webpage.\n"
          ]
        }
      ],
      "source": [
        "# Dataset Presentation\n",
        "# Data Collecting: Webscraping using 'requests' and 'BeautifulSoup'\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# URL of the website you want to scrape\n",
        "url = 'https://example.com/reviews'\n",
        "\n",
        "# Fetch the content of the webpage\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Suppose reviews are in paragraphs with class 'review-text'\n",
        "    reviews = [p.text for p in soup.find_all('p', class_='review-text')]\n",
        "else:\n",
        "    print(\"Failed to retrieve the webpage.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Social Media Data Collection\n",
        "# Using Tweepy for Twitter API:\n",
        "import tweepy\n",
        "\n",
        "# Your Twitter API credentials\n",
        "consumer_key = 'YOUR_CONSUMER_KEY'\n",
        "consumer_secret = 'YOUR_CONSUMER_SECRET'\n",
        "access_token = 'YOUR_ACCESS_TOKEN'\n",
        "access_token_secret = 'YOUR_ACCESS_TOKEN_SECRET'\n",
        "\n",
        "# Setting up the tweepy authorization\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "\n",
        "# Creating the API object\n",
        "api = tweepy.API(auth)\n",
        "\n",
        "# Searching for tweets related to a keyword (e.g., \"machine learning\")\n",
        "tweets = api.search(q=\"machine learning\", count=100)\n",
        "\n",
        "# Extracting text from tweets\n",
        "tweet_texts = [tweet.text for tweet in tweets]\n"
      ],
      "metadata": {
        "id": "TW8FLVvE41T0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Cleaning\n",
        "# Loading the collected data into a pandas DataFrame:\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming the data collected previously are in two lists: reviews and tweet_texts\n",
        "# We combine them here for demonstration purposes\n",
        "all_texts = reviews + tweet_texts\n",
        "df = pd.DataFrame(all_texts, columns=['text'])"
      ],
      "metadata": {
        "id": "C--wnfkw4-Vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle missing values:\n",
        "\n",
        "# Drop rows with missing text values\n",
        "df.dropna(subset=['text'], inplace=True)"
      ],
      "metadata": {
        "id": "slTFPLLT5NXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove duplicates:\n",
        "\n",
        "df.drop_duplicates(subset=['text'], inplace=True)"
      ],
      "metadata": {
        "id": "8OIuQ7CV5VqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the text data:\n",
        "\n",
        "import re\n",
        "import string\n",
        "\n",
        "def clean_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Remove extra whitespaces\n",
        "    text = text.strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "df['cleaned_text'] = df['text'].apply(clean_text)\n"
      ],
      "metadata": {
        "id": "9wTH7hNn5Z-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text and remove stop words:\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# If you haven't downloaded the stopwords dataset, do it once using:\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('punkt')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def tokenize_and_remove_stopwords(text):\n",
        "    words = word_tokenize(text)\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    return \" \".join(filtered_words)\n",
        "\n",
        "df['processed_text'] = df['cleaned_text'].apply(tokenize_and_remove_stopwords)\n"
      ],
      "metadata": {
        "id": "E8qduZuw5j_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment Analysis\n",
        "# NLTK for tokenization, stemmingm and lemmatization\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# If you haven't downloaded the WordNet dataset, do it once using:\n",
        "# nltk.download('wordnet')\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def stem_and_lemmatize(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in stemmed_tokens]\n",
        "    return ' '.join(lemmatized_tokens)\n",
        "\n",
        "df['lemmatized_text'] = df['processed_text'].apply(stem_and_lemmatize)"
      ],
      "metadata": {
        "id": "gcGTxikX5rLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TextBlob\n",
        "# Returns polarity and subjectivity scores\n",
        "\n",
        "from textblob import TextBlob\n",
        "\n",
        "def get_sentiment(text):\n",
        "    analysis = TextBlob(text)\n",
        "    return analysis.sentiment\n",
        "\n",
        "df['sentiment'] = df['lemmatized_text'].apply(get_sentiment)\n",
        "df[['polarity', 'subjectivity']] = pd.DataFrame(df['sentiment'].tolist(), index=df.index)\n",
        "df.drop(columns='sentiment', inplace=True)\n"
      ],
      "metadata": {
        "id": "rOXGakJq59wC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VADER\n",
        "# Designed for sentiment analysis, especially for social media texts\n",
        "\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "# If you haven't downloaded the vader_lexicon, do it once using:\n",
        "# nltk.download('vader_lexicon')\n",
        "\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "def get_vader_sentiment(text):\n",
        "    score = sia.polarity_scores(text)\n",
        "    return score['compound']\n",
        "\n",
        "df['vader_score'] = df['lemmatized_text'].apply(get_vader_sentiment)"
      ],
      "metadata": {
        "id": "ivawropo593N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fake News Detection\n",
        "# Data Preparation\n",
        "# '1' indicates fake news and '0' indicates genuine news\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Splitting the data into training and test sets\n",
        "X = df['lemmatized_text']\n",
        "y = df['label']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorizing the text data\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.7)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n"
      ],
      "metadata": {
        "id": "Tu3A12oO6Wkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training a Classifier using Scikit-Learn\n",
        "# Example using a simple Logistic Regression classifier\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "clf = LogisticRegression()\n",
        "clf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = clf.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "LjswnbNF6gqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Deep Learning Approach using TensorFlow\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "# Building a simple neural network model\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train_tfidf.shape[1],)),\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Training the model\n",
        "history = model.fit(X_train_tfidf.toarray(), y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate on test data\n",
        "loss, accuracy = model.evaluate(X_test_tfidf.toarray(), y_test)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "WU4iDEYw6pwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Results and Insights\n",
        "# Analyzing results using Pandas\n",
        "\n",
        "# Quick statistical overview of sentiment scores (from previous steps)\n",
        "df[['polarity', 'subjectivity', 'vader_score']].describe()"
      ],
      "metadata": {
        "id": "6s5u8aNJ6z-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of Sentiments\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Setting style\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Distribution of Polarity scores\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df['polarity'], kde=True)\n",
        "plt.title('Distribution of Polarity Scores')\n",
        "plt.xlabel('Polarity Score')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MM6ZwOh57gfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of VADER Scores\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df['vader_score'], kde=True, color='green')\n",
        "plt.title('Distribution of VADER Compound Scores')\n",
        "plt.xlabel('VADER Compound Score')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dfTgxVnn7omR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of Fake vs Genuine News\n",
        "\n",
        "plt.figure(figsize=(7, 5))\n",
        "sns.countplot(x='label', data=df)\n",
        "plt.title('Distribution of Fake vs Genuine News')\n",
        "plt.xlabel('Label (0: Genuine, 1: Fake)')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "l1n4173H7tAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Relationship between Sentiment and News Authenticity\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='label', y='polarity', data=df)\n",
        "plt.title('Polarity Distribution by News Authenticity')\n",
        "plt.xlabel('Label (0: Genuine, 1: Fake)')\n",
        "plt.ylabel('Polarity Score')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zqm9ONcG7yDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trends in Sentiments Over Time:\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Convert date column to datetime type (if it's not already)\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "# Group by date and take the average polarity\n",
        "df_date = df.groupby('date').mean().reset_index()\n",
        "\n",
        "# Plotting the trend\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.lineplot(x='date', y='polarity', data=df_date)\n",
        "plt.title('Trend of Average Polarity Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Average Polarity')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QaEWERR673eS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Future Predictions Visualization\n",
        "\n",
        "# Hypothetical future predictions\n",
        "future_dates = pd.date_range(start=df['date'].max(), periods=4, freq='M')\n",
        "future_polarity = [0.1, 0.15, 0.13, 0.2]  # These are just hypothetical values\n",
        "\n",
        "future_df = pd.DataFrame({\n",
        "    'date': future_dates,\n",
        "    'predicted_polarity': future_polarity\n",
        "})\n",
        "\n",
        "# Merging with the original data for a continuous plot\n",
        "df_date['predicted_polarity'] = df_date['polarity']  # Actual data\n",
        "merged_df = pd.concat([df_date, future_df], ignore_index=True)\n",
        "\n",
        "# Plotting the trend with future predictions\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.lineplot(x='date', y='predicted_polarity', data=merged_df, label='Predicted Polarity')\n",
        "sns.lineplot(x='date', y='polarity', data=df_date, label='Actual Polarity')\n",
        "plt.title('Trend of Polarity with Future Predictions')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Polarity')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tfrFO4yg8BOY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}